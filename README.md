Sentiment Aura â€” Real-Time Speech Sentiment Visualization
A full-stack real-time application that captures microphone audio, streams it to a transcription service, analyzes emotional sentiment + keywords via an AI model, and generates a live, reactive Perlin-noise â€œauraâ€ visualization based on the user's mood.

This project satisfies all requirements from the Memory Machines Live AI-Powered "Sentiment Aura" challenge.
It focuses on: full-stack orchestration, clean UI, real-time streaming, data-driven generative visuals, and robust async handling.

ğŸš€ Demo Video 
https://drive.google.com/file/d/1GXwCnS3Xus647uhCt8txLLgfcFvvkSTZ/view?usp=sharing

ğŸ“Œ Features
ğŸ¤ 1. Real-Time Speech Transcription

Captures microphone audio using Web Audio API.

Streams raw PCM audio to Deepgram via WebSocket.

Displays live rolling transcript (partial + final).

ğŸ§  2. AI Sentiment + Keywords Extraction

When a final transcript is received:

A backend edge function processes the text.

AI model returns:

sentiment_score (0â€“1)

sentiment_label ("positive", "neutral", "negative")

mood (e.g., "happy", "stressed", "calm")

keywords[] (3â€“5 extracted topics)

ğŸ¨ 3. Generative Perlin Noise Visualization

Built using p5.js inside React.

Visualization dynamically responds to:

Sentiment label â†’ color hue

Sentiment score â†’ motion + energy

Keyword count â†’ density + particle complexity

Smooth transitions (lerp) for a "calm, liquid-like" effect.

ğŸ§© 4. Modern UX Components

TranscriptDisplay: auto-scrolling, semi-transparent glass UI.

KeywordsDisplay: animated keyword pills with fade-in + glow.

Mood Badge: emoji + color-coded mood analyzer.

Controls: Start/Stop recording with status indicator.

ğŸ›¡ 5. Error & Edge Case Handling

Handles Deepgram disconnects (reconnect logic).

Handles AI API failures with fallbacks.

Prevents duplicate keywords.

Validates empty or unclear transcripts.

Displays toast notifications for rate-limit issues.

ğŸ— Architecture Overview
Frontend (React + TypeScript + p5.js)

Audio capture

WebSocket stream to Deepgram

Real-time UI components

Perlin-noise visualization

Backend (Edge Function / API)

/analyze-sentiment endpoint

Sends transcript â†’ AI Model â†’ structured JSON response

Returns:

{
  "sentiment_score": 0.87,
  "sentiment_label": "positive",
  "mood": "excited",
  "keywords": ["presentation", "success", "happy"]
}

External APIs

Deepgram â€” real-time transcription

AI Model (OpenAI / Gemini / Claude equivalent) â€” sentiment + keywords

ğŸ”„ Data Flow (End-to-End)

(Matches exactly the data flow in the PDF brief)

User clicks Start

React requests mic access

Audio streamed â†’ Deepgram WebSocket

Deepgram returns:

partial transcripts

final transcripts

On is_final: true:

Frontend POSTs text â†’ backend API

Backend performs AI sentiment analysis

AI returns structured JSON

UI updates:

aura visualization

keyword pills

mood badge

React re-renders everything smoothly

ğŸ›  Tech Stack
Frontend

React + TypeScript

Vite

p5.js (Perlin Noise visualization)

TailwindCSS

Axios (HTTP calls)

Web Audio API + WebSocket

Backend

Edge Function / API Route

AI model integration (Gemini / OpenAI-compatible)

Third-Party APIs

Deepgram (Transcription API)

ğŸ“¦ Project Structure
Sentiment_Analysis/
â”‚
â”œâ”€â”€ public/
â”‚   â””â”€â”€ robots.txt
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/       # AuraCanvas, KeywordsDisplay, SentimentMeter
â”‚   â”œâ”€â”€ hooks/            # useDeepgram, useSentimentAnalysis
â”‚   â”œâ”€â”€ pages/            # Index.tsx, NotFound.tsx
â”‚   â”œâ”€â”€ lib/              # utilities
â”‚   â”œâ”€â”€ types/            # TypeScript definitions
â”‚   â”œâ”€â”€ main.tsx          # entry file
â”‚   â””â”€â”€ index.css         # global styles
â”‚
â”œâ”€â”€ supabase/
â”‚   â””â”€â”€ functions/
â”‚       â””â”€â”€ analyze-sentiment/
â”‚           â””â”€â”€ index.ts  # backend sentiment logic
â”‚
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ tailwind.config.js
â””â”€â”€ README.md

ğŸ§ª Running Locally
1ï¸âƒ£ Install dependencies
npm install

2ï¸âƒ£ Add environment variables

Create a .env file:

VITE_DEEPGRAM_API_KEY=your_key_here
AI_API_KEY=your_ai_key

3ï¸âƒ£ Run development server
npm run dev


Project is available at:

ğŸ‘‰ http://localhost:5173

ğŸ“¤ Deployment

You can deploy the project on:

Vercel (recommended for frontend)

Netlify

Render / Railway for backend functions

Just set the environment variables inside the platform dashboard.

ğŸ“ Future Enhancements

Multi-speaker detection

Emotion time-series graph

Save conversation history

Animated 3D aura mode (Three.js)

WebRTC collaborative visualization

ğŸ‘¤ Author

Sneh Patil
Full-Stack Developer â€¢ AI Systems â€¢ Real-Time Applications
